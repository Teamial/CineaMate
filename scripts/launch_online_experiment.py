#!/usr/bin/env python3
"""
Online Experiment Launch Script

Launches a multi-armed bandit experiment with proper traffic allocation,
guardrails configuration, and monitoring setup. Creates experiment via API,
configures traffic split, and sets up automated decision making.

Features:
- Traffic allocation: 20% Thompson, 20% Îµ-greedy, 20% UCB, 20% control, 20% holdout
- Duration: 14 days (configurable)
- Guardrails: error rate, latency, concentration, reward drop
- Automated decision making: ship/iterate/kill logic
- Monitoring: real-time dashboard and alerts

Usage:
    python scripts/launch_online_experiment.py --name "Bandit Test v1" --duration 14
"""

import sys
import os
import argparse
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import requests
import json

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from backend.database import SessionLocal
from backend.models import Experiment, PolicyAssignment
from backend.ml.guardrails import GuardrailsEngine
from backend.ml.decision_engine import DecisionEngine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ExperimentLauncher:
    """Launcher for online bandit experiments"""
    
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.db = SessionLocal()
        self.guardrails_engine = GuardrailsEngine(self.db)
        self.decision_engine = DecisionEngine(self.db)
        
        # Default experiment configuration
        self.default_config = {
            'name': 'Bandit Experiment',
            'duration_days': 14,
            'traffic_pct': 0.8,  # 80% of traffic in experiment
            'default_policy': 'thompson',
            'notes': 'Automated bandit experiment launch',
            'policies': {
                'thompson': 0.25,  # 25% of experiment traffic
                'egreedy': 0.25,   # 25% of experiment traffic
                'ucb': 0.25,       # 25% of experiment traffic
                'control': 0.25    # 25% of experiment traffic
            },
            'guardrails': {
                'error_rate': 0.01,      # 1%
                'latency_p95': 120,      # 120ms
                'arm_concentration': 0.50,  # 50%
                'reward_drop': 0.05      # 5%
            },
            'decision_criteria': {
                'min_uplift': 0.03,      # 3%
                'min_confidence': 0.95,  # 95%
                'min_window_days': 7,    # 7 days
                'max_experiment_days': 14,  # 14 days
                'min_events_per_policy': 1000,  # 1000 events
                'significance_level': 0.05  # p < 0.05
            }
        }
    
    def launch_experiment(self, config: Dict[str, Any]) -> str:
        """
        Launch a new experiment
        
        Args:
            config: Experiment configuration
            
        Returns:
            Experiment ID
        """
        logger.info("Launching new bandit experiment")
        
        # Merge with default config
        experiment_config = {**self.default_config, **config}
        
        # Create experiment
        experiment_id = self._create_experiment(experiment_config)
        
        # Configure guardrails
        self._configure_guardrails(experiment_id, experiment_config['guardrails'])
        
        # Configure decision engine
        self._configure_decision_engine(experiment_id, experiment_config['decision_criteria'])
        
        # Set up monitoring
        self._setup_monitoring(experiment_id)
        
        # Log launch
        self._log_experiment_launch(experiment_id, experiment_config)
        
        logger.info(f"Experiment {experiment_id} launched successfully")
        return experiment_id
    
    def _create_experiment(self, config: Dict[str, Any]) -> str:
        """Create experiment via API"""
        logger.info("Creating experiment via API")
        
        # Calculate end date
        start_date = datetime.utcnow()
        end_date = start_date + timedelta(days=config['duration_days'])
        
        # Prepare experiment data
        experiment_data = {
            'name': config['name'],
            'start_at': start_date.isoformat(),
            'end_at': end_date.isoformat(),
            'traffic_pct': config['traffic_pct'],
            'default_policy': config['default_policy'],
            'notes': config['notes']
        }
        
        # Create experiment via API
        response = requests.post(
            f"{self.api_base_url}/api/experiments",
            json=experiment_data,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code != 201:
            raise Exception(f"Failed to create experiment: {response.text}")
        
        experiment_id = response.json()['id']
        logger.info(f"Created experiment {experiment_id}")
        
        return experiment_id
    
    def _configure_guardrails(self, experiment_id: str, guardrails_config: Dict[str, Any]):
        """Configure guardrails for the experiment"""
        logger.info(f"Configuring guardrails for experiment {experiment_id}")
        
        # Update guardrails engine thresholds
        self.guardrails_engine.update_thresholds(guardrails_config)
        
        logger.info(f"Guardrails configured: {guardrails_config}")
    
    def _configure_decision_engine(self, experiment_id: str, decision_config: Dict[str, Any]):
        """Configure decision engine for the experiment"""
        logger.info(f"Configuring decision engine for experiment {experiment_id}")
        
        # Update decision engine criteria
        self.decision_engine.update_criteria(decision_config)
        
        logger.info(f"Decision engine configured: {decision_config}")
    
    def _setup_monitoring(self, experiment_id: str):
        """Set up monitoring for the experiment"""
        logger.info(f"Setting up monitoring for experiment {experiment_id}")
        
        # In a real system, this would:
        # - Set up dashboard monitoring
        # - Configure alert channels
        # - Set up log aggregation
        # - Configure metrics collection
        
        logger.info(f"Monitoring setup complete for experiment {experiment_id}")
    
    def _log_experiment_launch(self, experiment_id: str, config: Dict[str, Any]):
        """Log experiment launch for audit trail"""
        launch_log = {
            'experiment_id': experiment_id,
            'launch_time': datetime.utcnow().isoformat(),
            'config': config,
            'launcher': 'automated_script'
        }
        
        logger.info(f"EXPERIMENT LAUNCH: {json.dumps(launch_log, indent=2)}")
    
    def get_experiment_status(self, experiment_id: str) -> Dict[str, Any]:
        """Get current status of an experiment"""
        logger.info(f"Getting status for experiment {experiment_id}")
        
        # Get experiment details
        experiment = self.db.query(Experiment).filter(
            Experiment.id == experiment_id
        ).first()
        
        if not experiment:
            raise ValueError(f"Experiment {experiment_id} not found")
        
        # Get guardrail status
        guardrail_summary = self.guardrails_engine.check_guardrails(experiment_id)
        
        # Get decision analysis
        try:
            decision_result = self.decision_engine.analyze_experiment(experiment_id)
            decision_analysis = decision_result.to_dict()
        except Exception as e:
            logger.warning(f"Failed to get decision analysis: {e}")
            decision_analysis = None
        
        # Get traffic allocation
        traffic_allocation = self._get_traffic_allocation(experiment_id)
        
        status = {
            'experiment_id': experiment_id,
            'name': experiment.name,
            'status': 'active' if not experiment.end_at else 'ended',
            'start_at': experiment.start_at.isoformat(),
            'end_at': experiment.end_at.isoformat() if experiment.end_at else None,
            'traffic_pct': experiment.traffic_pct,
            'traffic_allocation': traffic_allocation,
            'guardrail_status': guardrail_summary.to_dict(),
            'decision_analysis': decision_analysis,
            'last_updated': datetime.utcnow().isoformat()
        }
        
        return status
    
    def _get_traffic_allocation(self, experiment_id: str) -> Dict[str, Any]:
        """Get traffic allocation for an experiment"""
        # Get policy assignments
        assignments = self.db.query(PolicyAssignment).filter(
            PolicyAssignment.experiment_id == experiment_id
        ).all()
        
        if not assignments:
            return {}
        
        # Count assignments by policy
        policy_counts = {}
        for assignment in assignments:
            policy = assignment.policy
            policy_counts[policy] = policy_counts.get(policy, 0) + 1
        
        # Calculate percentages
        total_assignments = sum(policy_counts.values())
        policy_percentages = {
            policy: count / total_assignments
            for policy, count in policy_counts.items()
        }
        
        return {
            'total_assignments': total_assignments,
            'policy_percentages': policy_percentages,
            'policy_counts': policy_counts
        }
    
    def stop_experiment(self, experiment_id: str, reason: str = "Manual stop"):
        """Stop an experiment"""
        logger.info(f"Stopping experiment {experiment_id}: {reason}")
        
        # Stop experiment via API
        response = requests.post(
            f"{self.api_base_url}/api/experiments/{experiment_id}/stop",
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code != 200:
            raise Exception(f"Failed to stop experiment: {response.text}")
        
        logger.info(f"Experiment {experiment_id} stopped successfully")
    
    def list_experiments(self) -> List[Dict[str, Any]]:
        """List all experiments"""
        experiments = self.db.query(Experiment).all()
        
        experiment_list = []
        for experiment in experiments:
            experiment_list.append({
                'id': str(experiment.id),
                'name': experiment.name,
                'status': 'active' if not experiment.end_at else 'ended',
                'start_at': experiment.start_at.isoformat(),
                'end_at': experiment.end_at.isoformat() if experiment.end_at else None,
                'traffic_pct': experiment.traffic_pct,
                'default_policy': experiment.default_policy
            })
        
        return experiment_list

def main():
    """Main function for launching experiments"""
    parser = argparse.ArgumentParser(description='Launch online bandit experiment')
    parser.add_argument('--name', type=str, required=True,
                       help='Experiment name')
    parser.add_argument('--duration', type=int, default=14,
                       help='Experiment duration in days')
    parser.add_argument('--traffic', type=float, default=0.8,
                       help='Traffic percentage for experiment (0.0-1.0)')
    parser.add_argument('--policies', type=str, default='thompson,egreedy,ucb,control',
                       help='Comma-separated list of policies')
    parser.add_argument('--api-url', type=str, default='http://localhost:8000',
                       help='API base URL')
    parser.add_argument('--dry-run', action='store_true',
                       help='Show configuration without launching')
    
    args = parser.parse_args()
    
    # Parse policies
    policies = args.policies.split(',')
    
    # Create launcher
    launcher = ExperimentLauncher(args.api_url)
    
    # Prepare configuration
    config = {
        'name': args.name,
        'duration_days': args.duration,
        'traffic_pct': args.traffic,
        'policies': {policy: 1.0/len(policies) for policy in policies},
        'notes': f'Launched via script with {len(policies)} policies'
    }
    
    if args.dry_run:
        logger.info("DRY RUN - Configuration:")
        logger.info(json.dumps(config, indent=2))
        return
    
    try:
        # Launch experiment
        experiment_id = launcher.launch_experiment(config)
        
        logger.info("="*60)
        logger.info("EXPERIMENT LAUNCHED SUCCESSFULLY")
        logger.info("="*60)
        logger.info(f"Experiment ID: {experiment_id}")
        logger.info(f"Name: {args.name}")
        logger.info(f"Duration: {args.duration} days")
        logger.info(f"Traffic: {args.traffic*100:.1f}%")
        logger.info(f"Policies: {', '.join(policies)}")
        logger.info("="*60)
        logger.info("Next steps:")
        logger.info("1. Monitor experiment in dashboard")
        logger.info("2. Check guardrails status")
        logger.info("3. Review decision analysis")
        logger.info("4. Set up alerts and notifications")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"Failed to launch experiment: {e}")
        sys.exit(1)
    finally:
        launcher.db.close()

if __name__ == "__main__":
    main()
